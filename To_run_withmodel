
# 241106_CM005_unit10_32_upd
link_folder = r'E:\Data\analysis_cm005\from_masks'
link_file_mat = r'E:\Data\analysis_cm005\from_masks\241106_CM005_unit10_32_upd.mat'
link_save = r'E:\Data\analysis_cm005\from_masks\Save_241106_CM005_unit10_32_upd_no_binning'
tiff_file = r"E:\Data\241106_CM005\unit10\cor\1\unit10.tif"
mask_folder = r"E:\Data\analysis_cm005\from_masks\Save_241106_CM005_unit10_32_upd_binning\MasksBinary.tif"
output_csv = link_save + "\intensity_data.csv"


sigma_value = 10
deg_value = 5
fs_value = 52
fps = 52
height_value=0.56        # Minimum height of peaks (adjust as needed)
prominence_value=0.52   # Minimum prominence of peaks
distance_value=2
cutoff_value = 5
use_recommended_params = True  # Set to False to use global parameters
binning = False
mat = False
list_val = True
binning_factor = 2

cols_list = ['Mask_0','Mask_48', 'Mask_51', 'Mask_54','Mask_62']
#cfu_list = ['CFU_0','CFU_50', 'CFU_61', 'CFU_43']

# Step 1: Create masks from .mat file if applicable
if mat:
    mat = scipy.io.loadmat(link_file_mat)
    cfu_to_combined_tif_and_standard_plot(
        cfu_link=link_file_mat,
        output_tiff_path=link_save,
        output_html_path=link_save,
        cfu_global=list(mat.keys())[3],
        index_2=2
    )
    print('Masks created from .mat file')
else:
    print('No masks created')

# Step 2: Calculate intensities from mask TIFF file
output_csv = link_save + '\intensity_data'
intensity_data = calculate_intensities_from_mask_tiff(
    tiff_file=tiff_file,
    mask_tiff_file=mask_folder,
    output_csv=output_csv
)

# Step 3: Prepare and normalize intensity data
intensity_data_corr = pd.DataFrame()
for col in intensity_data.columns:
    intensity_data_corr[col] = intensity_data[col] - intensity_data[col].min() + 50
dff_data = calculate_dff(intensity_data_corr)

# Step 4: Filter masks if a list is provided
pd_data_corr = dff_data
if list_val:
    pd_data_corr = pd_data_corr[cols_list]
    print('Masks filtered by list')
else:
    print('All masks are in use')

# Step 5: Apply binning if enabled
if binning:
    pd_data_corr = bin_dataframe(pd_data_corr, bin_factor=binning_factor)
    fs_value = fs_value / binning_factor
    print('Binning used')
else:
    print('No binning used')

# Step 6: Calculate baselines and Î”F/F
pd_data_corr_baseline = calculate_baseline(pd_data_corr, sigma=sigma_value, deg=deg_value)
pd_data_corr_df = pd.DataFrame()
for col in pd_data_corr_baseline.columns:
    pd_data_corr_df[col] = (pd_data_corr[col] - pd_data_corr_baseline[col]) / pd_data_corr_baseline[col]
pd_data_corr_df_baselinetocheck = calculate_baseline(pd_data_corr_df, sigma=sigma_value, deg=deg_value)

# Step 7: Identify calm regions and recommend parameters
calm_regions, recommendations = run_analysis_pipeline(
    pd_data_corr_df, fs_value, calm_length=1, peak_threshold=1, output_dir=link_save
)

# Initialize peak_indices and smoothed_data
peak_indices = {}
smoothed_data = {}

# Step 8: Use recommended parameters or global defaults for peak detection
if use_recommended_params:
    print("Using recommended parameters for peak detection")
    for _, row in recommendations.iterrows():
        roi = row["ROI"]
        if pd.notna(row["Recommended_Height"]) and pd.notna(row["Recommended_Prominence"]):
            # Use ROI-specific recommended parameters
            peak_indices_tmp, smoothed_data_tmp, _ = detect_peaks_on_smoothed_data(
                pd_data_corr_df[[roi]],
                fs=fs_value,
                cutoff=cutoff_value,
                height=row["Recommended_Height"],
                prominence=row["Recommended_Prominence"],
                distance=distance_value
            )
            peak_indices[roi] = peak_indices_tmp[roi]
            smoothed_data[roi] = smoothed_data_tmp[roi]
        else:
            peak_indices[roi] = []
            smoothed_data[roi] = pd_data_corr_df[roi].values
else:
    print("Using global parameters for peak detection")
    peak_indices, smoothed_data, _ = detect_peaks_on_smoothed_data(
        pd_data_corr_df,
        fs=fs_value,
        cutoff=cutoff_value,
        height=height_value,
        prominence=prominence_value,
        distance=distance_value
    )
# Step 9: Model predictions on peak detection
# Ensure directories exist before saving files
predicted_output_dir = os.path.join(link_save, "Overlap/Predicted")
detected_output_dir = os.path.join(link_save, "Overlap/Detected")
model_path = r'E:\Data\analysis_cm005\from_masks\Training_random\random_forest_model_challenging_most_positivedata_risetime2.pkl'  # Specify your model path
predicted_peaks_df = detect_peaks_with_model(
    df=pd.DataFrame(smoothed_data),  # Replace with your DataFrame of intensities
    model_path=r'E:\Data\analysis_cm005\from_masks\Training_random\random_forest_model_challenging_most_positivedata_risetime2.pkl',  # Path to your trained model
    window_size=60,
    save_path=link_save + r"\predicted_peaks.csv")
# Step 10: Plot results (with model predictions)
plot_peaks_with_predictions(
    pd_data_corr_df, peak_indices, predicted_peaks_df,
    predicted_output_dir, fs_value
)

# Step 11: Save updated peak analysis results
update_peak_csv(
    pd_data_corr_df, 
    peak_indices, 
    predicted_peaks_df, 
    os.path.join(predicted_output_dir)
)

# Step 12: Save analysis and generate reports
save_cfu_analysis_with_report_v4(
    pd_data_corr_df, 
    smoothed_data, 
    pd_data_corr_df_baselinetocheck, 
    peak_indices, 
    predicted_peaks_df, 
    recommendations, 
    output_dir=link_save,
    link_folder=link_folder, 
    link_file_mat=link_file_mat, 
    link_save=link_save,
    sigma_value=sigma_value, 
    deg_value=deg_value, 
    fs_value=fs_value, 
    binning=binning, 
    binning_factor=binning_factor
)

# Step 13: Additional plots and analyses

os.makedirs(predicted_output_dir, exist_ok=True)
os.makedirs(detected_output_dir, exist_ok=True)
plot_combined_smoothed_and_peak_density_with_predictions(
    pd_data_corr_df, 
    smoothed_data, 
    peak_indices, 
    predicted_peaks_df, 
    baseline_dict = pd_data_corr_df_baselinetocheck,
    fps=fs_value, 
    output_path=link_save
)
plot_high_overlap_pairs(
    pd_data_corr_df, 
    smoothed_data, 
    {trace: predicted_peaks_df[predicted_peaks_df["Mask"] == trace]["Frame"].values for trace in pd_data_corr_df.columns}, 
    predicted_overlap_df, 
    overlap_threshold=80, 
    fps=fs_value, 
    output_dir=predicted_output_dir
)
calculate_and_save_predicted_peak_amplitudes(
    pd_data_corr_df, 
    predicted_peaks_df, 
    fs=fs_value, 
    output_dir=predicted_output_dir
)
overlap_df = compare_peak_times(
    peak_indices, 
    tolerance_frames=1, 
    fps=fs_value, 
    output_path=detected_output_dir
)
plot_high_overlap_pairs(
    pd_data_corr_df, 
    smoothed_data, 
    peak_indices, 
    overlap_df, 
    overlap_threshold=80, 
    fps=fs_value, 
    output_dir=detected_output_dir
)
calculate_and_save_peak_amplitudes_with_context(
    pd_data_corr_df, 
    peak_indices, 
    fs=fs_value, 
    output_dir=detected_output_dir, 
    context_before=5, 
    context_after=0.5
)
