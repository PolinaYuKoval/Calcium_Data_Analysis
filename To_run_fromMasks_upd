##################Data (run from .mat files)########################################################
####################################################################################################

# 241106_CM005_unit10_32_upd
link_folder = r'E:\Data\analysis_cm005\from_masks'
link_file_mat = r'E:\Data\analysis_cm005\from_masks\241106_CM005_unit10_32_upd.mat'
link_save = r'E:\Data\analysis_cm005\from_masks\Save_241106_CM005_unit10_32_upd_binning'
tiff_file = r"E:\Data\241106_CM005\unit10\cor\1\unit10.tif"
mask_folder = r"E:\Data\analysis_cm005\from_masks\Save_241106_CM005_unit10_32_upd_binning\MasksBinary.tif"
output_csv = link_save + "\intensity_data.csv"


sigma_value = 10
deg_value = 5
fs_value = 52
fps = 52
height_value=0.56        # Minimum height of peaks (adjust as needed)
prominence_value=0.52   # Minimum prominence of peaks
distance_value=2
cutoff_value = 5
use_recommended_params = True  # Set to False to use global parameters
binning = True
mat = True
list_val = True
binning_factor = 2

cols_list = ['Mask_0','Mask_48', 'Mask_51', 'Mask_54','Mask_62']
#cfu_list = ['CFU_0','CFU_50', 'CFU_61', 'CFU_43']

# Step 1: Create masks from .mat file if applicable
if mat:
    mat = scipy.io.loadmat(link_file_mat)
    cfu_to_combined_tif_and_standard_plot(
        cfu_link=link_file_mat,
        output_tiff_path=link_save,
        output_html_path=link_save,
        cfu_global=list(mat.keys())[3],
        index_2=2
    )
    print('Masks created from .mat file')
else:
    print('No masks created')

# Step 2: Calculate intensities from mask TIFF file
output_csv = link_save + '\intensity_data'
intensity_data = calculate_intensities_from_mask_tiff(
    tiff_file=tiff_file,
    mask_tiff_file=mask_folder,
    output_csv=output_csv
)

# Step 3: Prepare and normalize intensity data
intensity_data_corr = pd.DataFrame()
for col in intensity_data.columns:
    intensity_data_corr[col] = intensity_data[col] - intensity_data[col].min() + 50
dff_data = calculate_dff(intensity_data_corr)

# Step 4: Filter masks if a list is provided
pd_data_corr = dff_data
if list_val:
    pd_data_corr = pd_data_corr[cols_list]
    print('Masks filtered by list')
else:
    print('All masks are in use')

# Step 5: Apply binning if enabled
if binning:
    pd_data_corr = bin_dataframe(pd_data_corr, bin_factor=binning_factor)
    fs_value = fs_value / binning_factor
    print('Binning used')
else:
    print('No binning used')

# Step 6: Calculate baselines and Î”F/F
pd_data_corr_baseline = calculate_baseline(pd_data_corr, sigma=sigma_value, deg=deg_value)
pd_data_corr_df = pd.DataFrame()
for col in pd_data_corr_baseline.columns:
    pd_data_corr_df[col] = (pd_data_corr[col] - pd_data_corr_baseline[col]) / pd_data_corr_baseline[col]
pd_data_corr_df_baselinetocheck = calculate_baseline(pd_data_corr_df, sigma=sigma_value, deg=deg_value)

# Step 7: Identify calm regions and recommend parameters
calm_regions, recommendations, peak_indices = run_analysis_pipeline_with_recommended_params(
    pd_data_corr_df, fs_value, calm_length=1, peak_threshold=1, output_dir=link_save
)

# Step 8: Choose whether to use recommended parameters or global defaults
if use_recommended_params:
    print("Using recommended parameters for peak detection")
    peak_indices = {}
    smoothed_data = {}
    for _, row in recommendations.iterrows():
        roi = row["ROI"]
        if pd.notna(row["Recommended_Height"]) and pd.notna(row["Recommended_Prominence"]):
            peak_indices_tmp, smoothed_data_tmp, _ = detect_peaks_on_smoothed_data(
                pd_data_corr_df[[roi]],
                fs=fs_value,
                cutoff=cutoff_value,
                height=row["Recommended_Height"],
                prominence=row["Recommended_Prominence"],
                distance=distance_value
            )
            peak_indices[roi] = peak_indices_tmp[roi]
            smoothed_data[roi] = smoothed_data_tmp[roi]
        else:
            peak_indices[roi] = []
            smoothed_data[roi] = pd_data_corr_df[roi].values
else:
    print("Using global parameters for peak detection")
    peak_indices, smoothed_data, _ = detect_peaks_on_smoothed_data(
        pd_data_corr_df,
        fs=fs_value,
        cutoff=cutoff_value,
        height=height_value,
        prominence=prominence_value,
        distance=distance_value
    )

# Step 9: Plot results
plot_peaks_on_smoothed_data_with_baseline(
    pd_data_corr_df, smoothed_data, peak_indices, pd_data_corr_df_baselinetocheck,
    link_save + '\CFU_Peaks_calc_smooth', fs_value
)
plot_peak_density_per_cfu(peak_indices, fs=fs_value, output_dir=link_save + '\CFU_Peaks_dencity')

# Step 10: Save analysis and generate reports
save_cfu_analysis_with_report_v3(
    pd_data_corr_df, smoothed_data, pd_data_corr_df_baselinetoc
