def cfu_to_tif(cfu_link, cfu_global, path_to_save):
    """
    Converts a CFU MATLAB data to a TIFF image and saves it.
    """
    mat = scipy.io.loadmat(cfu_link)
    os.makedirs(path_to_save, exist_ok=True)
    cfu_i = mat[cfu_global]
    cfu_info_normalized = (cfu_i / np.max(cfu_i) * 255).astype(np.uint8)
    path_to_save = os.path.join(path_to_save, cfu_global + '.tiff')
    image = Image.fromarray(cfu_info_normalized)
    image.save(path_to_save)
    return path_to_save

def cfu_to_combined_tif_and_standard_plot(cfu_link, output_tiff_path, output_html_path, cfu_global='cfuInfo1', index_2=2):
    """
    Converts CFU MATLAB data to a combined single multi-mask binary TIFF file and generates an overlapped binary standard plot.

    Parameters:
        cfu_link (str): Path to the .mat file.
        output_tiff_path (str): Path to save the combined TIFF file.
        output_html_path (str): Path to save the overlapped HTML plot.
        cfu_global (str): Name of the global CFU variable in the .mat file.
        index_2 (int): Index for selecting the masks.
    """
    mat = scipy.io.loadmat(cfu_link)
    os.makedirs(output_tiff_path, exist_ok=True)
    os.makedirs(output_html_path, exist_ok=True)

    masks = []
    mask_names = []
    for i in range(mat[cfu_global].shape[0]):
        mask_array = mat[cfu_global][i][index_2]
        binary_mask = (mask_array > 0).astype(np.uint8)
        scaled_mask = binary_mask * 255

        masks.append(scaled_mask)
        mask_names.append(f"Mask_{i}")
    combined_tiff_path = os.path.join(output_tiff_path, 'MasksBinary.tif')
    tiff.imwrite(combined_tiff_path, np.stack(masks, axis=0).astype(np.uint8))
    print(f"Combined binary masks saved to {combined_tiff_path}")

def get_data_global(cfu_link, path_to_save, cfu_global = 'cfuInfo1'):
    """
    Converts a CFU MATLAB data to a TIFF image and saves it.
    """
    mat = scipy.io.loadmat(cfu_link)
    os.makedirs(path_to_save, exist_ok=True)
    df_data = {}
    df_data_df = {}
    mask_name = 'Mask_'
    for i in range(mat[cfu_global].shape[0]):
        df_data['Mask_' + str(i)] = mat['cfuInfo1'][i][4][0]
        df_data_df['Mask_' + str(i)] = mat['cfuInfo1'][i][5][0]
    #df_data = pd.DataFrame(df_data)
    #df_data_df = pd.DataFrame(df_data_df)
    return df_data, df_data_df
def df_trans_ren(df, global_coef):
    """
    Transposes a DataFrame and renames columns with a prefix.
    """
    df_transposed = df.T
    df_transposed.columns = [global_coef + f"{i+1}" for i in range(df_transposed.shape[1])]
    return df_transposed

def calculate_baseline(df, sigma=10, deg=5):
    """
    Calculates baselines for each column in the DataFrame.
    """
    baselines = {}
    for col in df.columns:
        smoothed_data = gaussian_filter(df[col].values, sigma=sigma)
        baselines[col] = peakutils.baseline(smoothed_data, deg=deg)
    return pd.DataFrame(baselines)

def plot_and_save(df, baselines, output_dir, combined_filename="all_cfus_combined_graph.html"):
    """
    Plots and saves individual CFU graphs and a combined graph.
    """
    os.makedirs(output_dir, exist_ok=True)
    fig_combined = go.Figure()
    for col in df.columns:
        cfu_data = df[col].values
        baseline = baselines[col].values
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=cfu_data, mode='lines', name=f"CFU {col}", line=dict(color="blue")))
        fig.add_trace(go.Scatter(y=baseline, mode='lines', name=f"Baseline {col}", line=dict(color="red", dash="dash")))
        fig.update_layout(
            title=f"Graph for CFU {col} with Baseline",
            xaxis_title="Index",
            yaxis_title="Intensity",
            template="plotly_white"
        )
        output_path = os.path.join(output_dir, f"cfu_{col}_graph.html")
        fig.write_html(output_path)
        print(f"Saved interactive graph for CFU {col} as {output_path}")
        fig_combined.add_trace(go.Scatter(y=cfu_data, mode='lines', name=f"CFU {col}"))
        fig_combined.add_trace(go.Scatter(y=baseline, mode='lines', name=f"Baseline {col}", line=dict(dash="dash")))
    combined_output_path = os.path.join(output_dir, combined_filename)
    fig_combined.update_layout(
        title="Combined Graph for All CFUs with Baselines",
        xaxis_title="Index",
        yaxis_title="Intensity",
        template="plotly_white"
    )
    fig_combined.write_html(combined_output_path)
    print(f"Saved combined graph as {combined_output_path}")
    
def identify_calm_region(signal, fs, calm_length=2, peak_threshold=0.1):
    """
    Identifies the calm region of specified length with minimal standard deviation.

    Parameters:
        signal (np.ndarray): 1D array of the signal intensities.
        fs (float): Sampling frequency in Hz (frames per second).
        calm_length (float): Length of the calm region in seconds.
        peak_threshold (float): Threshold for identifying peaks (optional).

    Returns:
        tuple: (start_index, end_index, calm_region_std) of the identified calm region.
    """
    calm_frames = int(calm_length * fs)
    peaks, _ = find_peaks(signal, height=peak_threshold)
    min_std = float('inf')
    best_start = None
    for start in range(0, len(signal) - calm_frames + 1):
        end = start + calm_frames
        region = signal[start:end]
        if any((start <= peak < end) for peak in peaks):
            continue
        region_std = np.std(region)
        if region_std < min_std:
            min_std = region_std
            best_start = start
    if best_start is not None:
        return best_start, best_start + calm_frames, min_std
    else:
        raise ValueError("No suitable calm region found.")


def identify_calm_regions_for_df(df, fs, calm_length=2, peak_threshold=0.1):
    """
    Identifies calm regions for each ROI in the DataFrame.

    Parameters:
        df (pd.DataFrame): DataFrame where each column represents a region of interest (ROI).
        fs (float): Sampling frequency in Hz (frames per second).
        calm_length (float): Length of the calm region in seconds.
        peak_threshold (float): Threshold for identifying peaks (optional).

    Returns:
        pd.DataFrame: DataFrame containing the calm region statistics for each ROI.
    """
    calm_regions = []
    for roi in df.columns:
        signal = df[roi].values
        try:
            start_idx, end_idx, calm_std = identify_calm_region(signal, fs, calm_length, peak_threshold)
            calm_regions.append({
                "ROI": roi,
                "Start_Index": start_idx,
                "End_Index": end_idx,
                "Calm_Std": calm_std
            })
        except ValueError:
            print(f"No suitable calm region found for ROI: {roi}")
            calm_regions.append({
                "ROI": roi,
                "Start_Index": None,
                "End_Index": None,
                "Calm_Std": None
            })
    return pd.DataFrame(calm_regions)

def recommend_peak_params(df, calm_regions_df, multiplier=3):
    """
    Recommends height and prominence parameters based on calm region statistics.

    Parameters:
        df (pd.DataFrame): DataFrame where each column represents an ROI.
        calm_regions_df (pd.DataFrame): DataFrame containing calm region statistics for each ROI.
        multiplier (float): Multiplier for standard deviation to recommend height and prominence.

    Returns:
        pd.DataFrame: DataFrame containing recommended parameters for each ROI.
    """
    recommendations = []
    for _, row in calm_regions_df.iterrows():
        roi = row["ROI"]
        start_idx = row["Start_Index"]
        end_idx = row["End_Index"]
        if pd.isna(start_idx) or pd.isna(end_idx):
            recommendations.append({
                "ROI": roi,
                "Recommended_Height": None,
                "Recommended_Prominence": None
            })
            continue
        calm_region = df[roi].iloc[start_idx:end_idx]
        calm_std = calm_region.std()
        calm_mean = calm_region.mean()
        recommended_height = calm_mean + multiplier * calm_std
        recommended_prominence = multiplier * calm_std
        recommendations.append({
            "ROI": roi,
            "Recommended_Height": recommended_height,
            "Recommended_Prominence": recommended_prominence
        })
    return pd.DataFrame(recommendations)
def run_analysis_pipeline(df, fs, calm_length=2, peak_threshold=0.1, multiplier=3, output_dir="output"):
    """
    Runs the analysis pipeline: identify calm regions and recommend peak parameters.

    Parameters:
        df (pd.DataFrame): DataFrame where each column represents an ROI.
        fs (float): Sampling frequency in Hz (frames per second).
        calm_length (float): Length of the calm region in seconds.
        peak_threshold (float): Threshold for identifying peaks (optional).
        multiplier (float): Multiplier for standard deviation to recommend height and prominence.
        output_dir (str): Directory to save output CSV files.

    Returns:
        tuple: (calm_regions_df, recommendations_df)
    """
    import os
    os.makedirs(output_dir, exist_ok=True)
    calm_regions_df = identify_calm_regions_for_df(df, fs, calm_length, peak_threshold)
    calm_regions_csv = os.path.join(output_dir + "\calm_regions.csv")
    calm_regions_df.to_csv(calm_regions_csv, index=False)
    print(f"Calm regions saved to {calm_regions_csv}")
    recommendations_df = recommend_peak_params(df, calm_regions_df, multiplier)
    recommendations_csv = os.path.join(output_dir + "\peak_detection_recommendations.csv")
    recommendations_df.to_csv(recommendations_csv, index=False)
    print(f"Peak recommendations saved to {recommendations_csv}")
    return calm_regions_df, recommendations_df

    
def detect_peaks(df, height=None, prominence=None, distance=None):
    """
    Detects peaks in each column of a DataFrame.
    
    Parameters:
        df (pd.DataFrame): DataFrame with Delta F/F data.
        height (float, optional): Minimum height of peaks.
        prominence (float, optional): Minimum prominence of peaks.
        distance (int, optional): Minimum distance between peaks.
        
    Returns:
        dict: A dictionary with column names as keys and detected peak indices as values.
        pd.DataFrame: A DataFrame with the number of peaks detected for each CFU.
    """
    peak_indices = {}
    peak_counts = {}

    for col in df.columns:
        data = df[col].values
        peaks, _ = find_peaks(data, height=height, prominence=prominence, distance=distance)
        peak_indices[col] = peaks
        peak_counts[col] = len(peaks)
    peak_counts_df = pd.DataFrame(list(peak_counts.items()), columns=["CFU", "Peak Count"])
    peak_counts_df.set_index("CFU", inplace=True)
    
    return peak_indices, peak_counts_df

def plot_peaks(df, peak_indices, output_dir):
    """
    Plots data with detected peaks for each CFU.
    """
    os.makedirs(output_dir, exist_ok=True)

    for col in df.columns:
        data = df[col].values
        peaks = peak_indices[col]
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=data, mode='lines', name=f"CFU {col}"))
        fig.add_trace(go.Scatter(
            x=peaks, y=data[peaks], mode='markers', name=f"Peaks {col}",
            marker=dict(color='red', size=8)
        ))
        fig.update_layout(
            title=f"Detected Peaks for CFU {col}",
            xaxis_title="Index",
            yaxis_title="Intensity",
            template="plotly_white"
        )
        output_path = os.path.join(output_dir, f"cfu_{col}_peaks.html")
        fig.write_html(output_path)
        print(f"Saved peak detection graph for CFU {col} to {output_path}")

def low_pass_filter(data, cutoff, fs, order=4):
    """
    Apply a low-pass Butterworth filter to the data.

    Parameters:
        data (np.ndarray): The input data to filter.
        cutoff (float): The cutoff frequency in Hz.
        fs (float): The sampling frequency in Hz.
        order (int): The order of the filter.
    
    Returns:
        np.ndarray: The filtered data.
    """
    nyquist = 0.5 * fs  # Nyquist frequency
    if not (0 < cutoff < nyquist):
        raise ValueError(f"Cutoff frequency must be between 0 and Nyquist frequency ({nyquist}). Provided: {cutoff}")
    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency
    b, a = butter(order, normal_cutoff, btype='low', analog=False)  # Design filter
    filtered_data = filtfilt(b, a, data)  # Apply filter
    return filtered_data


def detect_peaks_on_smoothed_data(df, fs=30, cutoff=5, height=None, prominence=None, distance=None):
    """
    Detect peaks on smoothed data using a low-pass filter for noise reduction.

    Parameters:
        df (pd.DataFrame): DataFrame with original Delta F/F data.
        fs (float): Sampling frequency in Hz (default 30 Hz).
        cutoff (float): Cutoff frequency for the low-pass filter in Hz (default 5 Hz).
        height (float, optional): Minimum height of peaks.
        prominence (float, optional): Minimum prominence of peaks.
        distance (int, optional): Minimum distance between peaks.
        
    Returns:
        dict: A dictionary with column names as keys and detected peak indices as values.
        dict: A dictionary with column names as keys and smoothed data as values.
        pd.DataFrame: A DataFrame with the number of peaks detected for each CFU.
    """
    peak_indices = {}
    smoothed_data_dict = {}
    peak_counts = {}

    for col in df.columns:
        smoothed_data = low_pass_filter(df[col].values, cutoff=cutoff, fs=fs)
        smoothed_data_dict[col] = smoothed_data
        peaks, _ = find_peaks(smoothed_data, height=height, prominence=prominence, distance=distance)
        peak_indices[col] = peaks
        peak_counts[col] = len(peaks)
    peak_counts_df = pd.DataFrame(list(peak_counts.items()), columns=["CFU", "Peak Count"])
    peak_counts_df.set_index("CFU", inplace=True)
    
    return peak_indices, smoothed_data_dict, peak_counts_df

def plot_peaks_on_smoothed_data_with_baseline(df, smoothed_data_dict, peak_indices, baseline_dict, output_dir, fps):
    """
    Plots smoothed data, original df, baseline, and detected peaks for each CFU in seconds.

    Parameters:
        df (pd.DataFrame): Original data for reference.
        smoothed_data_dict (dict): Dictionary of smoothed data.
        peak_indices (dict): Dictionary of detected peaks.
        baseline_dict (dict): Dictionary of baseline values for each CFU.
        output_dir (str): Directory to save the plots.
        fps (float): Frames per second of the data.
    """
    os.makedirs(output_dir, exist_ok=True)
    fig_combined = go.Figure()

    for col in df.columns:
        original_data = df[col].values
        smoothed_data = smoothed_data_dict[col]
        peaks = peak_indices[col]
        baseline = baseline_dict[col]
        time_seconds = np.arange(len(original_data)) / fps  # Convert frame numbers to seconds

        # Create the plot
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=time_seconds, y=original_data, mode='lines', name=f"Original CFU {col}", line=dict(color="blue")))
        fig.add_trace(go.Scatter(x=time_seconds, y=smoothed_data, mode='lines', name=f"Smoothed CFU {col}", line=dict(color="green")))
        fig.add_trace(go.Scatter(x=time_seconds, y=baseline, mode='lines', name=f"Baseline {col}", line=dict(color="orange", dash='dash')))
        fig.add_trace(go.Scatter(
            x=time_seconds[peaks], y=smoothed_data[peaks], mode='markers', name=f"Peaks {col}",
            marker=dict(color='red', size=8)
        ))
        fig.update_layout(
            title=f"Smoothed Data with Detected Peaks for CFU {col}",
            xaxis_title="Time (s)",
            yaxis_title="Intensity",
            template="plotly_white"
        )

        output_path = os.path.join(output_dir, f"cfu_{col}_smoothed_peaks.html")
        fig.write_html(output_path)
        print(f"Saved peak detection graph for CFU {col} to {output_path}")

        # Add to combined plot
        fig_combined.add_trace(go.Scatter(x=time_seconds, y=smoothed_data, mode='lines', name=f"Smoothed CFU {col}"))
        fig_combined.add_trace(go.Scatter(
            x=time_seconds[peaks], y=smoothed_data[peaks], mode='markers', name=f"Peaks {col} ({len(peaks)})"
        ))
        fig_combined.add_trace(go.Scatter(x=time_seconds, y=baseline, mode='lines', name=f"Baseline {col} ({col})"))

    combined_output_path = os.path.join(output_dir, "all_cfus_smoothed_peaks_combined.html")
    fig_combined.update_layout(
        title="Smoothed Data with Detected Peaks for All CFUs",
        xaxis_title="Time (s)",
        yaxis_title="Intensity",
        template="plotly_white"
    )
    fig_combined.write_html(combined_output_path)
    print(f"Saved combined peak detection graph to {combined_output_path}")


def plot_peak_density_per_cfu(peak_indices, fs, output_dir="peak_density_plots", bins=30):
    """
    Plots the density of detected peaks per second for each CFU and saves as HTML.

    Parameters:
        peak_indices (dict): A dictionary with CFU names as keys and detected peak indices as values.
        fs (float): Sampling frequency in Hz.
        output_dir (str): Directory to save the HTML plots.
        bins (int): Number of bins for the histogram.
    """
    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists

    for cfu, indices in peak_indices.items():
        # Convert peak indices to time (seconds)
        peak_times = np.array(indices) / fs
        peak_counts, bin_edges = np.histogram(peak_times, bins=bins)
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=bin_edges[:-1],
            y=peak_counts,
            width=(bin_edges[1] - bin_edges[0]),
            marker=dict(color="blue", opacity=0.7),
            name="Peak Density"
        ))
        fig.update_layout(
            title=f"Peak Density for {cfu}",
            xaxis_title="Time (s)",
            yaxis_title="Peak Density (Peaks per Bin)",
            template="plotly_white",
            bargap=0.1
        )
        output_path = os.path.join(output_dir, f"peak_density_{cfu}.html")
        fig.write_html(output_path)
        print(f"Saved peak density plot for {cfu} to {output_path}")
        

def calculate_intensities_from_mask_tiff(tiff_file, mask_tiff_file, output_csv):
    """
    Calculate frame-wise intensities for each mask from a single mask TIFF file and save as a DataFrame.

    Parameters:
        tiff_file (str): Path to the multi-frame TIFF file.
        mask_tiff_file (str): Path to the mask TIFF file containing multiple masks as frames.
        output_csv (str): Path to save the intensity DataFrame.

    Returns:
        pd.DataFrame: DataFrame containing frame-wise intensities for each mask.
    """
    os.makedirs(output_csv, exist_ok=True)
    # Load the TIFF data and mask file
    with tiff.TiffFile(tiff_file) as tif:
        tiff_data = tif.asarray()

    with tiff.TiffFile(mask_tiff_file) as mask_tif:
        mask_data = mask_tif.asarray()

    print(f"Loaded TIFF data with shape {tiff_data.shape} (frames, height, width)")
    print(f"Loaded mask data with shape {mask_data.shape} (masks, height, width)")

    # Ensure mask dimensions match the TIFF data dimensions
    if mask_data.shape[1:] != tiff_data.shape[1:]:
        raise ValueError(
            f"Mask shape {mask_data.shape[1:]} does not match TIFF shape {tiff_data.shape[1:]}"
        )

    num_frames = tiff_data.shape[0]
    intensity_data = pd.DataFrame(index=range(num_frames))

    # Calculate intensities for each mask
    for i, mask in enumerate(mask_data):
        mask_bool = mask.astype(bool)
        frame_intensities = [np.mean(frame[mask_bool]) for frame in tiff_data]
        intensity_data[f"Mask_{i}"] = frame_intensities

    # Save the intensity data to a CSV
    intensity_data.to_csv(output_csv+ '\intensity_data.csv', index=False)
    print(f"Saved intensity data to {output_csv}")

    return intensity_data

def crop_tiff_dimensions(tiff_data, crop_pixels=5):
    """
    Crop the second and third dimensions (height and width) of a TIFF file.

    Parameters:
        tiff_data (np.ndarray): Original TIFF data (frames, height, width).
        crop_pixels (int): Number of pixels to crop from each side.

    Returns:
        np.ndarray: Cropped TIFF data.
    """
    return tiff_data[:, crop_pixels:-crop_pixels, crop_pixels:-crop_pixels]


def calculate_dff(intensity_data):
    """
    Calculate Delta F/F for the given intensity data using column-wise minimum as baseline.

    Parameters:
        intensity_data (pd.DataFrame): Frame-wise intensity data.

    Returns:
        pd.DataFrame: DataFrame containing Delta F/F values.
    """
    baselines = intensity_data.min(axis=0)
    dff = (intensity_data - baselines) / baselines
    return dff

def plot_dff_and_peaks(dff_data, peak_indices, output_dir):
    """
    Plot Delta F/F data and detected peaks for each mask.

    Parameters:
        dff_data (pd.DataFrame): Delta F/F DataFrame.
        peak_indices (dict): Dictionary of detected peak indices.
        output_dir (str): Directory to save the plots.
    """
    os.makedirs(output_dir, exist_ok=True)
    fig_combined = go.Figure()

    for col in dff_data.columns:
        dff_values = dff_data[col].values
        peaks = peak_indices[col]
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=dff_values, mode='lines', name=f"Delta F/F {col}", line=dict(color="blue")))
        fig.add_trace(go.Scatter(
            x=peaks, y=dff_values[peaks], mode='markers', name=f"Peaks {col}",
            marker=dict(color='red', size=8)
        ))
        fig.update_layout(
            title=f"Delta F/F and Peaks for {col}",
            xaxis_title="Frame",
            yaxis_title="Delta F/F",
            template="plotly_white"
        )
        fig.write_html(os.path.join(output_dir, f"{col}_dff_peaks.html"))
        fig_combined.add_trace(go.Scatter(y=dff_values, mode='lines', name=f"Delta F/F {col}"))
        fig_combined.add_trace(go.Scatter(
            x=peaks, y=dff_values[peaks], mode='markers', name=f"Peaks {col} ({len(peaks)})"
        ))
    combined_output_path = os.path.join(output_dir, "_combined_dff_peaks.html")
    fig_combined.update_layout(
        title="Combined Delta F/F and Peaks for All Masks",
        xaxis_title="Frame",
        yaxis_title="Delta F/F",
        template="plotly_white"
    )
    fig_combined.write_html(combined_output_path)
    print(f"Saved combined Delta F/F and peaks plot to {combined_output_path}")

def plot_and_save_intensities(intensity_data, smoothed_data, peak_indices, output_dir):
    """
    Plot and save intensity and peak detection results.
    """
    os.makedirs(output_dir, exist_ok=True)
    fig_combined = go.Figure()

    for col in intensity_data.columns:
        original_data = intensity_data[col].values
        smoothed = smoothed_data[col]
        peaks = peak_indices[col]
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=original_data, mode='lines', name=f"Original {col}", line=dict(color="blue")))
        fig.add_trace(go.Scatter(y=smoothed, mode='lines', name=f"Smoothed {col}", line=dict(color="green")))
        fig.add_trace(go.Scatter(
            x=peaks, y=smoothed[peaks], mode='markers', name=f"Peaks {col}",
            marker=dict(color='red', size=8)
        ))
        fig.update_layout(
            title=f"Intensity and Peaks for {col}",
            xaxis_title="Frame",
            yaxis_title="Intensity",
            template="plotly_white"
        )
        fig.write_html(os.path.join(output_dir, f"{col}_peaks.html"))
        fig_combined.add_trace(go.Scatter(y=smoothed, mode='lines', name=f"Smoothed {col}"))
        fig_combined.add_trace(go.Scatter(
            x=peaks, y=smoothed[peaks], mode='markers', name=f"Peaks {col} ({len(peaks)})"
        ))
    combined_output_path = os.path.join(output_dir, "combined_peaks.html")
    fig_combined.update_layout(
        title="Combined Intensity and Peaks for All Masks",
        xaxis_title="Frame",
        yaxis_title="Intensity",
        template="plotly_white"
    )
    fig_combined.write_html(combined_output_path)
    print(f"Saved combined intensity and peaks plot to {combined_output_path}")


def plot_combined_smoothed_and_peak_density(
    df, smoothed_data_dict, peak_indices, fps, height_value, prominence_value, distance_value, sigma_value, output_path
):
    """
    Plots the smoothed data with detected peaks, and aligns it with a barplot of peak density.

    Parameters:
        df (pd.DataFrame): Original Delta F/F data.
        smoothed_data_dict (dict): Dictionary of smoothed data for each CFU.
        peak_indices (dict): Dictionary of detected peaks for each CFU.
        fps (float): Sampling frequency in Hz.
        height_value (float): Minimum height of peaks for detection.
        prominence_value (float): Minimum prominence of peaks.
        distance_value (int): Minimum distance between peaks.
        sigma_value (float): Smoothing coefficient.
        output_path (str): Path to save the combined plot.
    """
    output_path = output_path + '/combined_smoothed_peak_density'
    os.makedirs(output_path, exist_ok=True)
    
    for cfu in df.columns:
        original_data = df[cfu].values
        smoothed_data = smoothed_data_dict[cfu]
        peaks = peak_indices[cfu]
        time_seconds = np.arange(len(original_data)) / fps  # Convert frame numbers to seconds
        peak_times = np.array(peaks) / fps
        bin_edges = np.arange(0, time_seconds[-1] + 1, 1)  # Align bins to time axis
        peak_counts, bin_edges = np.histogram(peak_times, bins=bin_edges)
        fig = sp.make_subplots(
            rows=2, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.1,
            row_heights=[0.7, 0.3],
            subplot_titles=(f"Smoothed Data with Peaks for {cfu}", "Peak Density")
        )
        fig.add_trace(
            go.Scatter(x=time_seconds, y=original_data, mode='lines', name="Original Data", line=dict(color="blue", width=1)),
            row=1, col=1
        )
        fig.add_trace(
            go.Scatter(x=time_seconds, y=smoothed_data, mode='lines', name="Smoothed Data", line=dict(color="green", width=2)),
            row=1, col=1
        )
        fig.add_trace(
            go.Scatter(x=time_seconds[peaks], y=smoothed_data[peaks], mode='markers', name="Detected Peaks",
                       marker=dict(color='red', size=8)),
            row=1, col=1
        )
        fig.add_trace(
            go.Bar(
                x=bin_edges[:-1], y=peak_counts, width=1,  # Ensure bar width aligns with bins
                name="Peak Density", marker=dict(color="blue", opacity=0.7)
            ),
            row=2, col=1
        )
        fig.update_layout(
            title=f"Combined Smoothed Data and Peak Density for {cfu}",
            xaxis_title="Time (s)",
            yaxis=dict(title="Intensity"),
            yaxis2=dict(title="Peak Density (Peaks per Bin)"),
            template="plotly_white",
            legend_title="Input Parameters",
            annotations=[
                dict(
                    text=(
                        f"<b>Parameters:</b><br>"
                        f"FPS: {fps}<br>"
                        f"Height: {height_value}<br>"
                        f"Prominence: {prominence_value}<br>"
                        f"Distance: {distance_value}<br>"
                        f"Smoothing Sigma: {sigma_value}"
                    ),
                    x=1.1, y=1.2, showarrow=False, xref="paper", yref="paper", align="left"
                )
            ]
        )
        fig.write_html(output_path + f"/{cfu}.html")
        print(f"Saved combined plot for {cfu} to {output_path}")

def compare_peak_times(peak_indices, tolerance_frames, fps, output_path):
    """
    Compares whether different masks (CFUs) have peaks at the same time within a given tolerance
    and generates a heatmap showing the overlap.

    Parameters:
        peak_indices (dict): A dictionary with CFU names as keys and detected peak indices as values.
        tolerance_frames (int): Tolerance around the peak for comparison (in frames).
        fps (float): Frames per second, used to annotate the heatmap with time in seconds.
        output_path (str): Path to save the heatmap HTML file.
    """
    cfus = list(peak_indices.keys())
    num_cfus = len(cfus)
    overlap_matrix = np.zeros((num_cfus, num_cfus), dtype=int)
    for i, cfu_1 in enumerate(cfus):
        for j, cfu_2 in enumerate(cfus):
            if i <= j:  # Only compute upper triangle (matrix is symmetric)
                peaks_1 = np.array(peak_indices[cfu_1])
                peaks_2 = np.array(peak_indices[cfu_2])
                for peak_1 in peaks_1:
                    matches = np.any(np.abs(peaks_2 - peak_1) <= tolerance_frames)
                    overlap_matrix[i, j] += matches
    overlap_matrix = overlap_matrix + overlap_matrix.T - np.diag(overlap_matrix.diagonal())
    overlap_df = pd.DataFrame(overlap_matrix, index=cfus, columns=cfus)
    fig = px.imshow(
        overlap_df,
        labels=dict(x="CFU", y="CFU", color="Peak Overlap Count"),
        x=cfus,
        y=cfus,
        color_continuous_scale="Viridis",
        title=f"Peak Overlap Heatmap (Tolerance: +/- {tolerance_frames} frames)"
    )
    time_tolerance = tolerance_frames / fps
    fig.add_annotation(
        text=f"Time Tolerance: +/- {time_tolerance:.2f} seconds",
        xref="paper", yref="paper",
        x=1.1, y=1.1, showarrow=False
    )
    heatmap_path = f"{output_path}\peak_overlap_heatmap.html"
    fig.write_html(heatmap_path)
    print(f"Saved heatmap to {heatmap_path}")

    return overlap_df

def plot_high_overlap_pairs(df, smoothed_data_dict, peak_indices, overlap_df, overlap_threshold, fps, output_dir):
    """
    Identifies CFU pairs with more than a specified overlap threshold and plots their data together,
    highlighting regions with overlapped peaks.

    Parameters:
        df (pd.DataFrame): Original CFU data.
        smoothed_data_dict (dict): Dictionary of smoothed data for each CFU.
        peak_indices (dict): Dictionary of detected peaks for each CFU.
        overlap_df (pd.DataFrame): Overlap percentage DataFrame.
        overlap_threshold (float): Threshold for overlap percentage to consider pairs.
        fps (float): Frames per second of the data.
        output_dir (str): Directory to save the plots.
    """
    output_dir = output_dir + '/overlap'
    os.makedirs(output_dir, exist_ok=True)
    high_overlap_pairs = [
        (cfu_1, cfu_2) for cfu_1 in overlap_df.index for cfu_2 in overlap_df.columns
        if cfu_1 != cfu_2 and overlap_df.loc[cfu_1, cfu_2] > overlap_threshold
    ]

    for cfu_1, cfu_2 in high_overlap_pairs:
        original_data_1 = df[cfu_1].values
        smoothed_data_1 = smoothed_data_dict[cfu_1]
        peaks_1 = np.array(peak_indices[cfu_1])

        original_data_2 = df[cfu_2].values
        smoothed_data_2 = smoothed_data_dict[cfu_2]
        peaks_2 = np.array(peak_indices[cfu_2])

        time_seconds = np.arange(len(original_data_1)) / fps  # Convert frame numbers to seconds
        overlapping_peaks = [
            peak_1 for peak_1 in peaks_1 if np.any(np.abs(peaks_2 - peak_1) <= 4)
        ]
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=time_seconds, y=original_data_1, mode='lines',
            name=f"Original {cfu_1}", line=dict(color="blue", width=1)
        ))
        fig.add_trace(go.Scatter(
            x=time_seconds, y=smoothed_data_1, mode='lines',
            name=f"Smoothed {cfu_1}", line=dict(color="green", width=2)
        ))
        fig.add_trace(go.Scatter(
            x=time_seconds[peaks_1], y=smoothed_data_1[peaks_1], mode='markers',
            name=f"Peaks {cfu_1}", marker=dict(color='red', size=8)
        ))
        fig.add_trace(go.Scatter(
            x=time_seconds, y=original_data_2, mode='lines',
            name=f"Original {cfu_2}", line=dict(color="purple", width=1)
        ))
        fig.add_trace(go.Scatter(
            x=time_seconds, y=smoothed_data_2, mode='lines',
            name=f"Smoothed {cfu_2}", line=dict(color="orange", width=2)
        ))
        fig.add_trace(go.Scatter(
            x=time_seconds[peaks_2], y=smoothed_data_2[peaks_2], mode='markers',
            name=f"Peaks {cfu_2}", marker=dict(color='black', size=8)
        ))
        for overlap_peak in overlapping_peaks:
            overlap_time = time_seconds[overlap_peak]
            fig.add_vrect(
                x0=overlap_time - (4 / fps), x1=overlap_time + (4 / fps),
                fillcolor="rgba(255, 0, 0, 0.3)", layer="below", line_width=0
            )
        fig.update_layout(
            title=f"High Overlap Between {cfu_1} and {cfu_2} (> {overlap_threshold}%)",
            xaxis_title="Time (s)",
            yaxis_title="Intensity",
            template="plotly_white"
        )
        output_path = os.path.join(output_dir, f" overlap_{cfu_1}_{cfu_2}.html")
        fig.write_html(output_path)
        print(f"Saved high-overlap plot for {cfu_1} and {cfu_2} to {output_path}")

def bin_dataframe(df, bin_factor=2):
    """
    Bins a dataframe by averaging consecutive rows in groups of bin_factor.

    Parameters:
        df (pd.DataFrame): Input dataframe where rows represent frames and columns represent measurements (e.g., CFUs).
        bin_factor (int): Number of consecutive rows to average.

    Returns:
        pd.DataFrame: Binned dataframe with reduced row count.
    """
    num_rows = df.shape[0]
    if num_rows % bin_factor != 0:
        print(f"Warning: Number of rows ({num_rows}) is not divisible by bin_factor ({bin_factor}). "
              f"Truncating to {num_rows - (num_rows % bin_factor)} rows.")
        df = df.iloc[:num_rows - (num_rows % bin_factor)]
    binned_data = (
        df.values.reshape(-1, bin_factor, df.shape[1])
        .mean(axis=1)
    )
    binned_df = pd.DataFrame(binned_data, columns=df.columns)

    return binned_df

def get_mask_overlap_table(tiff1_path, tiff2_path, output_csv_path, threshold=0.5):
    """
    Computes the overlap between masks in two 3D TIFF files and generates a table of corresponding masks.

    Parameters:
        tiff1_path (str): Path to the first TIFF file.
        tiff2_path (str): Path to the second TIFF file.
        threshold (float): Minimum overlap ratio to consider masks as corresponding (default: 0.5).

    Returns:
        pd.DataFrame: DataFrame containing mask correspondences with overlap percentages.
    """
    masks1 = tiff.imread(tiff1_path)
    masks2 = tiff.imread(tiff2_path)
    if masks1.shape[1:] != masks2.shape[1:]:
        raise ValueError("Frames in the two TIFF files must have the same dimensions.")
    correspondences = []
    for i, mask1 in enumerate(masks1):
        for j, mask2 in enumerate(masks2):
            intersection = np.logical_and(mask1 > 0, mask2 > 0).sum()
            area_mask1 = (mask1 > 0).sum()
            area_mask2 = (mask2 > 0).sum()
            overlap_ratio = intersection / min(area_mask1, area_mask2)
            if overlap_ratio > threshold:
                correspondences.append({
                    "Mask_TIFF1": f"Mask_{i}",
                    "Mask_TIFF2": f"Mask_{j}",
                    "Overlap (%)": overlap_ratio * 100
                })
    overlap_table = pd.DataFrame(correspondences)
    overlap_table.to_csv(output_csv_path, index=False)
    print(f"Overlap table saved to {output_csv_path}")

    return overlap_table

def calculate_and_save_peak_amplitudes_with_context(df, peak_indices, fs, output_dir, context_before=5, context_after=0.5):
    """
    Calculate peak amplitudes using local minima and save peak context data, amplitudes, and stats.
    Generate a barplot with error bars for the amplitudes.
    """
    os.makedirs(output_dir, exist_ok=True)
    amplitudes = []
    context_data = pd.DataFrame()

    for col in df.columns:
        data = df[col].values
        peaks = peak_indices[col]
        col_amplitudes = []
        for peak in peaks:
            start_window = max(0, peak - 3)
            local_min = np.min(data[start_window:peak])
            amplitude = data[peak] - local_min
            col_amplitudes.append(amplitude)

            start_context = max(0, peak - context_before)
            end_context = min(len(data), peak + int(context_after * fs))
            context_df = pd.DataFrame({
                "Frame": np.arange(start_context, end_context),
                "Time (s)": np.arange(start_context, end_context) / fs,
                "Intensity": data[start_context:end_context],
                "Mask": col,
                "Peak Frame": peak,
                "Peak Amplitude": amplitude
            })
            context_data = pd.concat([context_data, context_df], ignore_index=True)

        amplitudes.extend([{"Mask": col, "Amplitude": amp} for amp in col_amplitudes])

    amplitude_df = pd.DataFrame(amplitudes)
    amplitude_csv_path = os.path.join(output_dir, "peak_amplitudes.csv")
    amplitude_df.to_csv(amplitude_csv_path, index=False)
    print(f"Saved peak amplitudes to {amplitude_csv_path}")

    context_csv_path = os.path.join(output_dir, "peak_context_data.csv")
    context_data.to_csv(context_csv_path, index=False)
    print(f"Saved peak context data to {context_csv_path}")
    stats = amplitude_df.groupby("Mask").Amplitude.agg(['mean', 'std', 'count'])
    stats["std"] = stats["std"].fillna(0)
    stats = stats.rename(columns={"mean": "Mean", "std": "Std", "count": "Count"})
    x = stats.index
    y = stats["Mean"]
    error = stats["Std"]
    fig = px.bar(
        stats.reset_index(),
        x="Mask",
        y="Mean",
        error_y="Std",
        labels={"Mean": "Mean Amplitude", "Mask": "ROI"},
        title="Barplot of Peak Amplitudes with Error Bars",
        color="Mask",
        template="plotly_white"
    )
    barplot_path = os.path.join(output_dir, "amplitude_barplot_with_error_bars.html")
    fig.write_html(barplot_path)
    print(f"Barplot with error bars saved to {barplot_path}")

def run_analysis_pipeline_with_recommended_params(
    df, fs, calm_length=2, peak_threshold=0.1, multiplier=3, output_dir="output"
):
    """
    Runs the analysis pipeline: identify calm regions, recommend peak parameters,
    and detect peaks using those parameters.

    Parameters:
        df (pd.DataFrame): DataFrame where each column represents an ROI.
        fs (float): Sampling frequency in Hz (frames per second).
        calm_length (float): Length of the calm region in seconds.
        peak_threshold (float): Threshold for identifying peaks (optional).
        multiplier (float): Multiplier for standard deviation to recommend height and prominence.
        output_dir (str): Directory to save output CSV files.

    Returns:
        tuple: (calm_regions_df, recommendations_df, peak_indices)
    """
    os.makedirs(output_dir, exist_ok=True)
    calm_regions_df = identify_calm_regions_for_df(df, fs, calm_length, peak_threshold)
    calm_regions_csv = os.path.join(output_dir, "calm_regions.csv")
    calm_regions_df.to_csv(calm_regions_csv, index=False)
    print(f"Calm regions saved to {calm_regions_csv}")

    recommendations_df = recommend_peak_params(df, calm_regions_df, multiplier)
    recommendations_csv = os.path.join(output_dir, "peak_detection_recommendations.csv")
    recommendations_df.to_csv(recommendations_csv, index=False)
    print(f"Peak recommendations saved to {recommendations_csv}")
    peak_indices = {}
    for _, row in recommendations.iterrows():
        roi = row["ROI"]
        if pd.notna(row["Recommended_Height"]) and pd.notna(row["Recommended_Prominence"]):
            peak_indices_tmp, smoothed_data_tmp, _ = detect_peaks_on_smoothed_data(
                pd_data_corr_df[[roi]], 
                fs=fs_value, 
                cutoff=cutoff_value, 
                height=row["Recommended_Height"], 
                prominence=row["Recommended_Prominence"], 
                distance=distance_value
            )
            peak_indices[roi] = peak_indices_tmp[roi]
            smoothed_data[roi] = smoothed_data_tmp[roi]

    return calm_regions_df, recommendations_df, peak_indices

def save_cfu_analysis_with_report_v3(
    df, smoothed_data_dict, baseline_dict, peak_indices, recommendations_df, output_dir,
    link_folder, link_file_mat, link_save, sigma_value, deg_value, fs_value, binning, binning_factor
):
    """
    Saves the CFU analysis results to CSV files and generates a detailed TXT report,
    including recommended parameters for each mask.

    Parameters:
        df (pd.DataFrame): Original CFU DataFrame.
        smoothed_data_dict (dict): Dictionary containing smoothed data for each CFU.
        baseline_dict (dict): Dictionary containing baseline data for each CFU.
        peak_indices (dict): Dictionary containing indices of detected peaks for each CFU.
        recommendations_df (pd.DataFrame): DataFrame of recommended parameters.
        output_dir (str): Directory to save the CSV files and report.
    """
    os.makedirs(output_dir, exist_ok=True)
    cfu_analysis_df = pd.DataFrame()
    for cfu in df.columns:
        original_data = df[cfu]
        smoothed_data = smoothed_data_dict[cfu]
        baseline_data = baseline_dict[cfu]
        peak_mask = np.zeros(len(smoothed_data), dtype=int)
        peak_mask[peak_indices[cfu]] = 1  # Mark peaks as 1, others as 0

        cfu_data = pd.DataFrame({
            "CFU": cfu,
            "Original CFU Data": original_data,
            "Smoothed CFU Data": smoothed_data,
            "Baseline CFU Data": baseline_data,
            "Detected Peaks (Smoothed)": peak_mask
        })
        cfu_analysis_df = pd.concat([cfu_analysis_df, cfu_data], ignore_index=True)
    cfu_analysis_csv_path = os.path.join(output_dir, "cfu_analysis.csv")
    cfu_analysis_df.to_csv(cfu_analysis_csv_path, index=False)
    print(f"Saved CFU analysis to {cfu_analysis_csv_path}")

    peaks_data = []
    for cfu, indices in peak_indices.items():
        peaks_data.append({"CFU": cfu, "Detected Peak Count": len(indices)})

    peaks_df = pd.DataFrame(peaks_data)
    peaks_csv_path = os.path.join(output_dir, "detected_peaks_summary.csv")
    peaks_df.to_csv(peaks_csv_path, index=False)
    print(f"Saved detected peaks summary to {peaks_csv_path}")

    report_path = os.path.join(output_dir, "analysis_report.txt")
    with open(report_path, "w") as report_file:
        report_file.write("Analysis Report\n")
        report_file.write("=" * 40 + "\n\n")
        report_file.write("Links and Paths:\n")
        report_file.write(f"Data Folder: {link_folder}\n")
        report_file.write(f"MAT File: {link_file_mat}\n")
        report_file.write(f"Save Folder: {link_save}\n\n")
        report_file.write("Coefficients Used:\n")
        report_file.write(f"Smoothing Sigma Value: {sigma_value}\n")
        report_file.write(f"Baseline Degree: {deg_value}\n")
        report_file.write(f"Frames Per Second (FPS): {fs_value}\n")
        report_file.write(f"Binning: {binning}\n\n")
        report_file.write(f"Binning Factor: {binning_factor}\n\n")
        col_list = df.columns.tolist()
        report_file.write(f"Total Length of Data (Mask_1): {len(df[col_list[0]])}\n")
        report_file.write("\nNumber of Peaks per Mask:\n")
        for cfu, indices in peak_indices.items():
            report_file.write(f"- {cfu}: {len(indices)} peaks\n")
        report_file.write("\nRecommended Peak Detection Parameters:\n")
        for _, row in recommendations_df.iterrows():
            report_file.write(f"- {row['ROI']}:\n")
            report_file.write(f"  Height: {row['Recommended_Height']}\n")
            report_file.write(f"  Prominence: {row['Recommended_Prominence']}\n")
        report_file.write("\nFiles Saved:\n")
        report_file.write(f"- CFU Analysis: {cfu_analysis_csv_path}\n")
        report_file.write(f"- Peaks Summary: {peaks_csv_path}\n")
        report_file.write(f"- Recommendations: {os.path.join(output_dir, 'peak_detection_recommendations.csv')}\n")
        report_file.write("\nDate and Time of Analysis:\n")
        report_file.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report_file.write("\nOutput Folder:\n")
        report_file.write(f"{output_dir}\n")

    print(f"Saved analysis report to {report_path}")

def calculate_nrmse(original, smoothed):
    """
    Calculate the Normalized Root Mean Square Error (NRMSE) between the original and smoothed data.
    """
    rmse = np.sqrt(np.mean((original - smoothed) ** 2))
    data_range = np.max(original) - np.min(original)
    nrmse = rmse / data_range
    return nrmse

def detect_nrmse(df, smoothed_df, fs, height=None, prominence=None, distance=None):
    """
    Detect peaks using already smoothed data and evaluate smoothing quality with NRMSE.

    Parameters:
        df (pd.DataFrame): Original DataFrame with raw signals.
        smoothed_df (pd.DataFrame): Smoothed DataFrame for peak detection.
        fs (float): Sampling frequency in Hz.
        height (float): Minimum peak height.
        prominence (float): Minimum peak prominence.
        distance (int): Minimum distance between peaks.

    Returns:
        dict: Peak indices for each signal.
        pd.DataFrame: NRMSE values for each signal.
    """
    nrmse_values = {}
    for col in df.columns:
        original_signal = df[col].values
        smoothed_signal = smoothed_df[col].values
        nrmse = calculate_nrmse(original_signal, smoothed_signal)
        nrmse_values[col] = nrmse
    nrmse_df = pd.DataFrame.from_dict(nrmse_values, orient='index', columns=["NRMSE"])
    return  nrmse_df

def generate_datasets(output_dir, num_training=100, num_testing=20, fs_range=(30, 64), **kwargs):
    """
    Generate datasets with varying frame rates and save as CSVs.

    Parameters:
        output_dir (str): Directory to save datasets.
        num_training (int): Number of training files.
        num_testing (int): Number of testing files.
        fs_range (tuple): Range of sampling frequencies (Hz).
    """
    training_dir = os.path.join(output_dir, "Training")
    testing_dir = os.path.join(output_dir, "Testing")
    os.makedirs(training_dir, exist_ok=True)
    os.makedirs(testing_dir, exist_ok=True)

    for dataset, num_files in zip([training_dir, testing_dir], [num_training, num_testing]):
        for i in range(num_files):
            signals = []
            fs = np.random.uniform(*fs_range)
            for _ in range(5):  # Each file contains 5 signals
                signal, _ = generate_synthetic_calcium_signal(fs=fs, **kwargs)
                signals.append(signal)
            pd.DataFrame(signals).T.to_csv(os.path.join(dataset, f"Dataset_{i+1}.csv"), index=False)

def generate_negative_calcium_signal(length=1000, noise_level=0.1):
    """
    Generate a negative calcium signal with no peaks and only noise.

    Parameters:
        length (int): Length of the signal (number of time points).
        noise_level (float): Relative noise level (fraction of signal amplitude).

    Returns:
        np.ndarray: Negative calcium signal (noise only).
    """
    # Generate a flat signal with noise
    noise = noise_level * np.random.normal(size=length)
    return noise

def load_data(directory, label=1):
    """
    Load synthetic signals from a directory and label them.

    Parameters:
        directory (str): Path to the directory containing CSV files.
        label (int): Label for the data (1 = peaks, 0 = no peaks).

    Returns:
        np.ndarray: Flattened features.
        np.ndarray: Corresponding labels.
    """
    features = []
    labels = []
    for file in os.listdir(directory):
        if file.endswith(".csv"):
            filepath = os.path.join(directory, file)
            data = pd.read_csv(filepath).values.T  # Transpose to get signals as rows
            features.extend(data)
            labels.extend([label] * data.shape[0])
    return np.array(features), np.array(labels)

def prepare_features_from_peaks(df, peak_indices, window_size=10):
    """
    Prepare features for each detected peak based on the signal around the peak.

    Parameters:
        df (pd.DataFrame): DataFrame containing the signal traces (Delta F/F).
        peak_indices (dict): Dictionary containing detected peak indices for each trace.
        window_size (int): Number of frames before and after the peak to include in the feature.

    Returns:
        np.ndarray: Feature matrix for the detected peaks.
        list: List of corresponding trace/column names.
    """
    features = []
    trace_names = []
    for trace_name, peaks in peak_indices.items():
        signal = df[trace_name].values
        for peak in peaks:
            start = max(0, peak - window_size)
            end = min(len(signal), peak + window_size)
            # Extract the window and pad with zeros if necessary
            window = np.zeros(2 * window_size)
            valid_range = slice(window_size - (peak - start), window_size + (end - peak))
            window[valid_range] = signal[start:end]
            features.append(window)
            trace_names.append(trace_name)

    return np.array(features), trace_names

def detect_peaks_with_model(df, model_path, window_size=10, save_path="predicted_peaks.csv"):
    """
    Detect peaks in intensity traces using a trained model.

    Parameters:
        df (pd.DataFrame): DataFrame with intensity traces, where each column is a mask.
        model_path (str): Path to the trained Random Forest model.
        window_size (int): Number of frames before and after the peak to include in the feature.
        save_path (str): Path to save the detected peaks as a CSV file.

    Returns:
        pd.DataFrame: DataFrame containing detected peaks with mask names and coordinates.
    """
    model = joblib.load(model_path)

    all_peaks = []
    for mask_name in df.columns:
        signal = df[mask_name].values
        features = []
        frame_indices = []

        # Create a feature vector for each frame in the signal
        for frame in range(len(signal)):
            start = max(0, frame - window_size)
            end = min(len(signal), frame + window_size + 1)
            window = np.zeros(2 * window_size + 1)
            valid_range = slice(window_size - (frame - start), window_size + (end - frame))
            window[valid_range] = signal[start:end]
            features.append(window)
            frame_indices.append(frame)

        features = np.array(features)
        
        if features.size == 0:
            continue  # Skip if no features are available

        # Model prediction
        predictions = model.predict(features)

        # Store valid predicted peaks
        for frame, prediction in zip(frame_indices, predictions):
            if prediction == 1:  # Valid peak predicted by the model
                all_peaks.append({"Mask": mask_name, "Frame": frame})

    # Convert results to a DataFrame and save
    peaks_df = pd.DataFrame(all_peaks)
    if not peaks_df.empty:
        peaks_df.to_csv(save_path, index=False)
        print(f"Predicted peaks saved to {save_path}")
    else:
        print("No peaks detected and predicted by the model.")

    return peaks_df


def calculate_peak_overlap(peaks_df, tolerance=1):
    """
    Calculate overlap between peaks for each pair of masks.

    Parameters:
        peaks_df (pd.DataFrame): DataFrame with 'Mask' and 'Frame' columns.
        tolerance (int): Number of frames for peak overlap tolerance.

    Returns:
        pd.DataFrame: Overlap matrix between masks.
    """
    masks = peaks_df['Mask'].unique()
    overlap_matrix = pd.DataFrame(0, index=masks, columns=masks)

    for mask_a in masks:
        peaks_a = peaks_df[peaks_df['Mask'] == mask_a]['Frame'].values
        for mask_b in masks:
            if mask_a == mask_b:
                overlap_matrix.loc[mask_a, mask_b] = len(peaks_a)
            else:
                peaks_b = peaks_df[peaks_df['Mask'] == mask_b]['Frame'].values
                overlap_count = sum(
                    any(abs(peak_a - peaks_b) <= tolerance) for peak_a in peaks_a
                )
                overlap_matrix.loc[mask_a, mask_b] = overlap_count

    return overlap_matrix

def plot_overlap_heatmap(overlap_matrix, output_path):
    """
    Generate a heatmap for the overlap matrix.

    Parameters:
        overlap_matrix (pd.DataFrame): Matrix of overlaps between masks.
        output_path (str): Path to save the heatmap HTML file.
    """
    fig = px.imshow(
        overlap_matrix,
        labels=dict(x="Mask", y="Mask", color="Overlap Count"),
        title="Peak Overlap Heatmap",
        color_continuous_scale="Viridis"
    )
    heatmap_path = f"{output_path}/overlap_heatmap.html"
    fig.write_html(heatmap_path)
    print(f"Heatmap saved to {heatmap_path}")

def generate_synthetic_calcium_signal(length=1000, num_peaks=20, fs=30, noise_level=0.1, decay_time=0.5):
    """
    Generate a synthetic calcium signal with exponential peaks and noise.

    Parameters:
        length (int): Length of the signal (number of time points).
        num_peaks (int): Number of peaks to generate.
        fs (float): Sampling frequency (frames per second).
        noise_level (float): Relative noise level (fraction of the signal's amplitude).
        decay_time (float): Decay time in seconds.

    Returns:
        np.ndarray: Synthetic calcium signal.
        np.ndarray: Indices of the generated peaks.
    """
    signal = np.zeros(length)
    rise_time = int(0.1 * fs)
    decay_time_frames = int(decay_time * fs)  # Frames for the decay
    peak_indices = np.random.choice(np.arange(rise_time, length - decay_time_frames), size=num_peaks, replace=False)
    peak_indices.sort()

    for peak in peak_indices:
        rise = np.linspace(0, 1, rise_time)
        decay = np.exp(-np.linspace(0, 5, decay_time_frames))
        full_peak = np.concatenate([rise, decay])
        amplitude = np.random.uniform(0.1, 2)
        signal[peak - rise_time:peak + decay_time_frames] += amplitude * full_peak[:min(len(full_peak), length - (peak - rise_time))]

    noise = noise_level * np.random.normal(size=signal.shape)
    signal += noise
    return signal, peak_indices
def calculate_and_save_predicted_peak_amplitudes(df, predictions_df, fs, output_dir):
    """
    Calculate and save amplitudes of predicted peaks for each trace.

    Parameters:
        df (pd.DataFrame): DataFrame of signal traces.
        predictions_df (pd.DataFrame): DataFrame with predicted peaks.
        fs (float): Sampling frequency.
        output_dir (str): Directory to save results.

    Returns:
        None
    """
    os.makedirs(output_dir, exist_ok=True)
    amplitudes = []

    for trace in df.columns:
        data = df[trace].values
        # Updated column names to match the provided DataFrame
        predicted_indices = predictions_df[(predictions_df["Mask"] == trace)]["Frame"].values
        for peak in predicted_indices:
            local_min = np.min(data[max(0, peak - 3):peak])  # Find local minimum near the peak
            amplitude = data[peak] - local_min
            amplitudes.append({
                "Trace": trace,
                "Frame": peak,
                "Amplitude": amplitude
            })

    # Save results to a CSV file
    amplitudes_df = pd.DataFrame(amplitudes)
    output_file = os.path.join(output_dir, "predicted_peak_amplitudes.csv")
    amplitudes_df.to_csv(output_file, index=False)
    print(f"Predicted peak amplitudes saved to {output_file}")


def generate_challenging_negative_signal(length=1000, fs=30, noise_level=0.2):
    """
    Generate a challenging negative calcium signal with pseudo-bursts and sinusoidal noise.
    """
    signal = np.zeros(length)
    num_pseudo_bursts = np.random.randint(1, 3)
    for _ in range(num_pseudo_bursts):
        start = np.random.randint(0, length - fs)
        pseudo_burst = np.sin(np.linspace(0, np.pi, fs)) * np.random.uniform(0.1, 0.3)
        end = min(length, start + len(pseudo_burst))
        signal[start:end] += pseudo_burst[:end - start]

    t = np.linspace(0, length / fs, length)
    sinusoidal_noise = np.sin(2 * np.pi * 0.1 * t) * 0.2
    signal += sinusoidal_noise

    noise = noise_level * np.random.normal(size=signal.shape)
    signal += noise
    return signal

def load_data_from_generated_signals(num_positive, num_negative, length=1000, fs=30):
    """
    Generate and load data for training.
    """
    positive_signals = [generate_challenging_positive_signal(length=length, fs=fs) for _ in range(num_positive)]
    negative_signals = [generate_challenging_negative_signal(length=length, fs=fs) for _ in range(num_negative)]

    positive_labels = [1] * len(positive_signals)
    negative_labels = [0] * len(negative_signals)

    features = np.array(positive_signals + negative_signals)
    labels = np.array(positive_labels + negative_labels)

    return features, labels

def train_model(features, labels, model_save_path):
    """
    Train a Random Forest model on the provided features and labels.
    """
    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)

    print(f"Training Set Size: {X_train.shape}")
    print(f"Validation Set Size: {X_val.shape}")

    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_classifier.fit(X_train, y_train)

    y_pred = rf_classifier.predict(X_val)
    print("Validation Set Evaluation:")
    print("Classification Report:\n", classification_report(y_val, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_val, y_pred))

    joblib.dump(rf_classifier, model_save_path)
    print(f"Model saved to {model_save_path}")

def plot_peaks_with_predictions(df, peak_indices, predictions_df, output_dir, fps):
    """
    Plot calcium signal with detected peaks (red) and predicted peaks (green).

    Parameters:
        df (pd.DataFrame): DataFrame containing the calcium signal traces.
        peak_indices (dict): Dictionary of detected peak indices for each signal.
        predictions_df (pd.DataFrame): DataFrame of predicted peaks from the model.
        output_dir (str): Path to save the plots.
        fps (float): Sampling frequency for time axis.
    """
    os.makedirs(output_dir, exist_ok=True)

    for trace in df.columns:
        signal = df[trace].values
        detected_peaks = peak_indices.get(trace, [])

        # Use 'Mask' instead of 'Trace'
        predicted_peaks = predictions_df[predictions_df["Mask"] == trace]
        predicted_indices = predicted_peaks["Frame"].values

        time = np.arange(len(signal)) / fps
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=time, y=signal, mode="lines", name="Signal"))
        fig.add_trace(go.Scatter(
            x=time[detected_peaks],
            y=signal[detected_peaks],
            mode="markers",
            marker=dict(color="red", size=8),
            name="Detected Peaks"
        ))
        fig.add_trace(go.Scatter(
            x=time[predicted_indices],
            y=signal[predicted_indices],
            mode="markers",
            marker=dict(color="green", size=10),
            name="Predicted Peaks"
        ))
        fig.update_layout(
            title=f"Peaks for {trace}",
            xaxis_title="Time (s)",
            yaxis_title="Signal Intensity",
            template="plotly_white"
        )
        output_path = os.path.join(output_dir, f"{trace}_peaks_with_predictions.html")
        fig.write_html(output_path)
        print(f"Saved plot for {trace} to {output_path}")

        

def save_cfu_analysis_with_report_v4(
    df, smoothed_data_dict, baseline_dict, peak_indices, predictions_df, recommendations_df, output_dir,
    link_folder, link_file_mat, link_save, sigma_value, deg_value, fs_value, binning, binning_factor
):
    """
    Enhanced CFU analysis with additional details for both detected and predicted peaks, and overlap statistics.

    Parameters:
        df (pd.DataFrame): Original CFU DataFrame.
        smoothed_data_dict (dict): Dictionary containing smoothed data for each CFU.
        baseline_dict (dict): Dictionary containing baseline data for each CFU.
        peak_indices (dict): Dictionary containing indices of detected peaks for each CFU.
        predictions_df (pd.DataFrame): DataFrame containing model-predicted peaks.
        recommendations_df (pd.DataFrame): DataFrame of recommended parameters.
        output_dir (str): Directory to save the CSV files and report.
    """
    os.makedirs(output_dir, exist_ok=True)
    cfu_analysis_df = pd.DataFrame()
    for cfu in df.columns:
        original_data = df[cfu]
        smoothed_data = smoothed_data_dict[cfu]
        baseline_data = baseline_dict[cfu]
        detected_peaks = np.zeros(len(smoothed_data), dtype=int)
        detected_peaks[peak_indices[cfu]] = 1  # Mark detected peaks
        predicted_peaks = np.zeros(len(smoothed_data), dtype=int)
        if not predictions_df.empty:
            predicted_indices = predictions_df[predictions_df["Mask"] == cfu]["Frame"].values
            predicted_peaks[predicted_indices] = 1

        cfu_data = pd.DataFrame({
            "CFU": cfu,
            "Original CFU Data": original_data,
            "Smoothed CFU Data": smoothed_data,
            "Baseline CFU Data": baseline_data,
            "Detected Peaks (Smoothed)": detected_peaks,
            "Predicted Peaks": predicted_peaks
        })
        cfu_analysis_df = pd.concat([cfu_analysis_df, cfu_data], ignore_index=True)

    cfu_analysis_csv_path = os.path.join(output_dir, "cfu_analysis_with_predictions.csv")
    cfu_analysis_df.to_csv(cfu_analysis_csv_path, index=False)
    print(f"Saved CFU analysis to {cfu_analysis_csv_path}")
    detected_peaks_summary = [{"CFU": cfu, "Detected Peak Count": len(peak_indices[cfu])} for cfu in df.columns]
    predicted_peaks_summary = predictions_df.groupby("Mask")["Frame"].count().reset_index()
    predicted_peaks_summary.columns = ["CFU", "Predicted Peak Count"]

    overlap_counts = [
        {
            "CFU": cfu,
            "Overlap Count": len(
                set(peak_indices[cfu]).intersection(
                    predictions_df[predictions_df["Mask"] == cfu]["Frame"].values
                )
            )
        }
        for cfu in df.columns
    ]

    detected_peaks_df = pd.DataFrame(detected_peaks_summary)
    detected_peaks_df.to_csv(os.path.join(output_dir, "detected_peaks_summary.csv"), index=False)
    predicted_peaks_summary.to_csv(os.path.join(output_dir, "predicted_peaks_summary.csv"), index=False)
    overlap_counts_df = pd.DataFrame(overlap_counts)
    overlap_counts_df.to_csv(os.path.join(output_dir, "overlap_summary.csv"), index=False)

    print(f"Saved detected peaks summary to {os.path.join(output_dir, 'detected_peaks_summary.csv')}")
    print(f"Saved predicted peaks summary to {os.path.join(output_dir, 'predicted_peaks_summary.csv')}")
    print(f"Saved overlap summary to {os.path.join(output_dir, 'overlap_summary.csv')}")
    report_path = os.path.join(output_dir, "analysis_report_v4.txt")
    with open(report_path, "w") as report_file:
        report_file.write("Analysis Report (v4)\n")
        report_file.write("=" * 40 + "\n\n")
        report_file.write("Links and Paths:\n")
        report_file.write(f"Data Folder: {link_folder}\n")
        report_file.write(f"MAT File: {link_file_mat}\n")
        report_file.write(f"Save Folder: {link_save}\n\n")
        report_file.write("Coefficients Used:\n")
        report_file.write(f"Smoothing Sigma Value: {sigma_value}\n")
        report_file.write(f"Baseline Degree: {deg_value}\n")
        report_file.write(f"Frames Per Second (FPS): {fs_value}\n")
        report_file.write(f"Binning: {binning}\n")
        report_file.write(f"Binning Factor: {binning_factor}\n")
        report_file.write(f"Total Length of Data (Mask_0): {len(df['Mask_0'])}\n\n")
        report_file.write("Number of Peaks per Mask:\n")
        for cfu, indices in peak_indices.items():
            report_file.write(f"- {cfu}: {len(indices)} detected peaks\n")
        report_file.write("\nRecommended Peak Detection Parameters:\n")
        for _, row in recommendations_df.iterrows():
            report_file.write(f"- {row['ROI']}:\n")
            report_file.write(f"  Height: {row['Recommended_Height']}\n")
            report_file.write(f"  Prominence: {row['Recommended_Prominence']}\n")
        report_file.write("\nOverlap Statistics:\n")
        report_file.write(f"Total Detected Peaks: {detected_peaks_df['Detected Peak Count'].sum()}\n")
        report_file.write(f"Total Predicted Peaks: {predicted_peaks_summary['Predicted Peak Count'].sum()}\n")
        report_file.write(f"Total Overlap Peaks: {overlap_counts_df['Overlap Count'].sum()}\n\n")
        report_file.write("Files Saved:\n")
        report_file.write(f"- CFU Analysis: {cfu_analysis_csv_path}\n")
        report_file.write(f"- Detected Peaks Summary: {os.path.join(output_dir, 'detected_peaks_summary.csv')}\n")
        report_file.write(f"- Predicted Peaks Summary: {os.path.join(output_dir, 'predicted_peaks_summary.csv')}\n")
        report_file.write(f"- Overlap Summary: {os.path.join(output_dir, 'overlap_summary.csv')}\n")
        report_file.write(f"- Recommendations: {os.path.join(output_dir, 'peak_detection_recommendations.csv')}\n")
        report_file.write("\nDate and Time of Analysis:\n")
        report_file.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report_file.write("\nOutput Folder:\n")
        report_file.write(f"{output_dir}\n")

    print(f"Saved analysis report to {report_path}")

def plot_combined_smoothed_and_peak_density_with_predictions(
    df, smoothed_data_dict, peak_indices, predicted_peaks_df, baseline_dict, fps, output_path
):
    """
    Plots smoothed data, detected peaks, predicted peaks, and baselines combined with peak density.

    Parameters:
        df (pd.DataFrame): Original data for reference.
        smoothed_data_dict (dict): Dictionary of smoothed data.
        peak_indices (dict): Dictionary of detected peaks.
        predicted_peaks_df (pd.DataFrame): DataFrame with predicted peaks and their indices.
        baseline_dict (dict): Dictionary of baseline values for each CFU.
        fps (float): Frames per second of the data.
        output_path (str): Path to save the plots.
    """
    output_path = os.path.join(output_path, 'combined_smoothed_peak_density_with_predictions')
    os.makedirs(output_path, exist_ok=True)

    combined_fig = go.Figure()

    for trace in df.columns:
        original_data = df[trace].values
        smoothed_data = smoothed_data_dict[trace]
        baseline = baseline_dict[trace]
        detected_indices = peak_indices.get(trace, [])
        predicted_indices = predicted_peaks_df[predicted_peaks_df["Mask"] == trace]["Frame"].values
        time = np.arange(len(original_data)) / fps
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=time, y=smoothed_data, mode="lines", name="Smoothed Data", line=dict(color="green", width=2)))
        fig.add_trace(go.Scatter(x=time, y=original_data, mode="lines", name="Original Data", line=dict(color="blue", width=1)))
        fig.add_trace(go.Scatter(
            x=time, y=baseline, mode="lines", name="Baseline", line=dict(color="orange", dash="dash")
        ))
        fig.add_trace(go.Scatter(
            x=time[detected_indices], y=smoothed_data[detected_indices],
            mode="markers", name="Detected Peaks", marker=dict(color="red", size=8)
        ))
        fig.add_trace(go.Scatter(
            x=time[predicted_indices], y=smoothed_data[predicted_indices],
            mode="markers", name="Predicted Peaks", marker=dict(color="green", size=8)
        ))
        fig.update_layout(
            title=f"Smoothed Data with Peaks and Baseline for {trace}",
            xaxis_title="Time (s)",
            yaxis_title="Intensity",
            template="plotly_white"
        )
        individual_output_path = os.path.join(output_path, f"{trace}_smoothed_density_with_predictions.html")
        fig.write_html(individual_output_path)
        print(f"Saved combined plot with predictions for {trace} to {individual_output_path}")
        combined_fig.add_trace(go.Scatter(x=time, y=smoothed_data, mode="lines", name=f"Smoothed {trace}"))
        combined_fig.add_trace(go.Scatter(
            x=time[detected_indices], y=smoothed_data[detected_indices],
            mode="markers", name=f"Detected Peaks {trace}", marker=dict(color="red", size=8)
        ))
        combined_fig.add_trace(go.Scatter(
            x=time[predicted_indices], y=smoothed_data[predicted_indices],
            mode="markers", name=f"Predicted Peaks {trace}", marker=dict(color="green", size=8)
        ))
        combined_fig.add_trace(go.Scatter(
            x=time, y=baseline, mode="lines", name=f"Baseline {trace}", line=dict(color="orange", dash="dash")
        ))
    combined_output_path = os.path.join(output_path, "all_cfus_smoothed_density_with_predictions_combined.html")
    combined_fig.update_layout(
        title="Smoothed Data with Detected and Predicted Peaks for All CFUs",
        xaxis_title="Time (s)",
        yaxis_title="Intensity",
        template="plotly_white"
    )
    combined_fig.write_html(combined_output_path)
    print(f"Saved combined peak detection graph to {combined_output_path}")
def update_peak_csv(df, detected_peaks, predicted_peaks_df, output_path):
    """
    Update the peak CSV with overlap counts and predicted peaks.

    Parameters:
        df (pd.DataFrame): Original DataFrame containing calcium signals.
        detected_peaks (dict): Dictionary of detected peak indices for each signal.
        predicted_peaks_df (pd.DataFrame): DataFrame of predicted peaks.
        output_csv (str): Path to save the updated peaks file.

    Returns:
        None
    """
    overlap_data = []
    overlap_count = 0

    for mask in df.columns:  # Use "Mask" instead of "Trace"
        detected_indices = set(detected_peaks.get(mask, []))
        predicted_indices = set(
            predicted_peaks_df[(predicted_peaks_df["Mask"] == mask)]["Frame"].values
        )
        overlap = detected_indices.intersection(predicted_indices)
        overlap_count += len(overlap)

        overlap_data.append({
            "Mask": mask,
            "Detected Peaks": len(detected_indices),
            "Predicted Peaks": len(predicted_indices),
            "Overlap": len(overlap),
        })
    output_csv = os.path.join(output_path, '/peaks_with_predictions.csv')
    overlap_df = pd.DataFrame(overlap_data)
    overlap_df.to_csv(output_csv, index=False)
    print(f"Updated peak analysis saved to {output_csv}")
    print(f"Total Overlap Count: {overlap_count}")
